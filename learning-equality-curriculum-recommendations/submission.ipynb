{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:46:03.243400Z",
     "iopub.status.busy": "2023-03-12T02:46:03.242948Z",
     "iopub.status.idle": "2023-03-12T02:46:03.272922Z",
     "shell.execute_reply": "2023-03-12T02:46:03.271923Z",
     "shell.execute_reply.started": "2023-03-12T02:46:03.243358Z"
    }
   },
   "outputs": [],
   "source": [
    "# # train/val スプリット。channel で分割する。channel 間で共通の content もあるため、共通項が最小になるような channel で分ける\n",
    "# topics_val = topics[[\"id\", \"channel\"]]\n",
    "# topics_val = topics_val.merge(\n",
    "#     correlations, how=\"inner\", left_on=\"id\", right_on=\"topic_id\"\n",
    "# )\n",
    "\n",
    "# channel_val = topics_val.groupby(\"channel\").agg(list).reset_index()\n",
    "# channel_val[\"content_ids\"] = channel_val[\"content_ids\"].apply(\n",
    "#     lambda x: list(np.unique(np.concatenate([x_.split() for x_ in x])))\n",
    "# )\n",
    "\n",
    "\n",
    "# def iou(a, b):\n",
    "#     return len(set(a).intersection(set(b))) / len(set(a + b))\n",
    "\n",
    "\n",
    "# ious = np.zeros((len(channel_val), len(channel_val)))\n",
    "\n",
    "# for i in range(len(channel_val)):\n",
    "#     for j in range(i):\n",
    "#         iou_ij = iou(channel_val[\"content_ids\"][i], channel_val[\"content_ids\"][j])\n",
    "#         ious[i, j] = iou_ij\n",
    "#         ious[j, i] = iou_ij\n",
    "\n",
    "# G = nx.Graph(ious)\n",
    "\n",
    "# components = [list(k) for k in nx.connected_components(G)]\n",
    "# for i, c in enumerate(components):\n",
    "#     print(f\"Component #{i} of length {len(c)}\")\n",
    "\n",
    "# # 0.8:0.2 に分割。171 * 0.2 は約 34 なので、#23- を val として分ける\n",
    "# channel_val[\"fold\"] = 0\n",
    "# channel_val.loc[sum(components[23:], []), \"fold\"] = 1\n",
    "\n",
    "# print(len(channel_val[channel_val[\"fold\"] == 0]))  # 137\n",
    "# print(len(channel_val[channel_val[\"fold\"] == 1]))  # 34\n",
    "\n",
    "# topics_without_source = topics[topics[\"category\"] != \"source\"]\n",
    "\n",
    "# topics_pipeline_train = topics[\n",
    "#     topics[\"channel\"].isin(channel_val[channel_val[\"fold\"] == 0][\"channel\"])\n",
    "# ]\n",
    "# topics_pipeline_val = topics_without_source[\n",
    "#     topics_without_source[\"channel\"].isin(\n",
    "#         channel_val[channel_val[\"fold\"] == 1][\"channel\"]\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# topics_pipeline_train.to_csv(\"topics_pipeline_train.csv\", index=False)\n",
    "# topics_pipeline_val.to_csv(\"topics_pipeline_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:46:03.276739Z",
     "iopub.status.busy": "2023-03-12T02:46:03.275315Z",
     "iopub.status.idle": "2023-03-12T02:46:03.284759Z",
     "shell.execute_reply": "2023-03-12T02:46:03.283712Z",
     "shell.execute_reply.started": "2023-03-12T02:46:03.276703Z"
    }
   },
   "outputs": [],
   "source": [
    "# Docker 上で実行するときのみ使用。Kaggle Notebooks 上で実行するときはコメントアウト\n",
    "# !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:46:03.287265Z",
     "iopub.status.busy": "2023-03-12T02:46:03.286468Z",
     "iopub.status.idle": "2023-03-12T02:46:34.933670Z",
     "shell.execute_reply": "2023-03-12T02:46:34.932525Z",
     "shell.execute_reply.started": "2023-03-12T02:46:03.287231Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ../input/sentence-transformers/sentence_transformers-2.2.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:46:34.936814Z",
     "iopub.status.busy": "2023-03-12T02:46:34.936381Z",
     "iopub.status.idle": "2023-03-12T02:46:49.709371Z",
     "shell.execute_reply": "2023-03-12T02:46:49.708221Z",
     "shell.execute_reply.started": "2023-03-12T02:46:34.936769Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.optimize import minimize\n",
    "from sentence_transformers import (\n",
    "    CrossEncoder,\n",
    "    InputExample,\n",
    "    SentenceTransformer,\n",
    "    losses,\n",
    ")\n",
    "from sentence_transformers.cross_encoder.evaluation import (\n",
    "    CEBinaryClassificationEvaluator,\n",
    ")\n",
    "from sentence_transformers.evaluation import BinaryClassificationEvaluator\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:46:49.713927Z",
     "iopub.status.busy": "2023-03-12T02:46:49.713230Z",
     "iopub.status.idle": "2023-03-12T02:47:10.443826Z",
     "shell.execute_reply": "2023-03-12T02:47:10.442774Z",
     "shell.execute_reply.started": "2023-03-12T02:46:49.713884Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_INPUT = \"../input/learning-equality-curriculum-recommendations\"\n",
    "PATH_TO_INPUT_VAL = \"../input/learning-equality-curriculum-recommendations-val\"\n",
    "PATH_TO_OUTPUT = \".\"\n",
    "\n",
    "TOPICS = \"topics.csv\"\n",
    "TOPICS_PIPELINE_TRAIN = \"topics_pipeline_train.csv\"\n",
    "TOPICS_PIPELINE_VAL = \"topics_pipeline_val.csv\"\n",
    "CONTENT = \"content.csv\"\n",
    "CORRELATIONS = \"correlations.csv\"\n",
    "SAMPLE_SUBMISSION = \"sample_submission.csv\"\n",
    "\n",
    "PATH_TO_RETRIEVER_WEIGHTS = \"../input/all-minilm-l6-v2-retriever\"\n",
    "PATH_TO_RERANKER_WEIGHTS = \"../input/all-minilm-l6-v2-reranker\"\n",
    "\n",
    "topics = pd.read_csv(os.path.join(PATH_TO_INPUT, TOPICS))\n",
    "topics_df = pd.read_csv(os.path.join(PATH_TO_INPUT, TOPICS), index_col=0)\n",
    "topics_pipeline_train = pd.read_csv(\n",
    "    os.path.join(PATH_TO_INPUT_VAL, TOPICS_PIPELINE_TRAIN)\n",
    ")\n",
    "topics_pipeline_val = pd.read_csv(os.path.join(PATH_TO_INPUT_VAL, TOPICS_PIPELINE_VAL))\n",
    "content = pd.read_csv(os.path.join(PATH_TO_INPUT, CONTENT))\n",
    "correlations = pd.read_csv(os.path.join(PATH_TO_INPUT, CORRELATIONS))\n",
    "sample_submission = pd.read_csv(os.path.join(PATH_TO_INPUT, SAMPLE_SUBMISSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:47:10.445713Z",
     "iopub.status.busy": "2023-03-12T02:47:10.445310Z",
     "iopub.status.idle": "2023-03-12T02:47:10.454038Z",
     "shell.execute_reply": "2023-03-12T02:47:10.452714Z",
     "shell.execute_reply.started": "2023-03-12T02:47:10.445674Z"
    }
   },
   "outputs": [],
   "source": [
    "class Topic:\n",
    "    def __init__(self, topic_id):\n",
    "        self.id = topic_id\n",
    "\n",
    "    @property\n",
    "    def parent(self):\n",
    "        parent_id = topics_df.loc[self.id].parent\n",
    "        if pd.isna(parent_id):\n",
    "            return None\n",
    "        else:\n",
    "            return Topic(parent_id)\n",
    "\n",
    "    @property\n",
    "    def ancestors(self):\n",
    "        ancestors = []\n",
    "        parent = self.parent\n",
    "        while parent is not None:\n",
    "            if parent.title:\n",
    "                ancestors.append(parent)\n",
    "            parent = parent.parent\n",
    "        return ancestors\n",
    "\n",
    "    def get_breadcrumbs(self, separator=\" >> \", include_self=True, include_root=True):\n",
    "        ancestors = self.ancestors\n",
    "        if include_self:\n",
    "            ancestors = [self] + ancestors\n",
    "        if not include_root:\n",
    "            ancestors = ancestors[:-1]\n",
    "        return separator.join(reversed([a.title for a in ancestors]))\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return topics_df.loc[self.id][name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:47:10.456266Z",
     "iopub.status.busy": "2023-03-12T02:47:10.455617Z",
     "iopub.status.idle": "2023-03-12T02:47:10.474358Z",
     "shell.execute_reply": "2023-03-12T02:47:10.473176Z",
     "shell.execute_reply.started": "2023-03-12T02:47:10.456228Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特徴量を作るための関数\n",
    "def create_feature_for_topics(topics):\n",
    "    topics = topics[topics[\"has_content\"]]\n",
    "\n",
    "    topics = topics.merge(\n",
    "        topics_df[\"description\"].rename(\"parent_description\"),\n",
    "        how=\"left\",\n",
    "        left_on=\"parent\",\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    children_description = topics.merge(\n",
    "        topics_df[[\"description\", \"parent\"]].rename(\n",
    "            columns={\"description\": \"children_description\"}\n",
    "        ),\n",
    "        how=\"left\",\n",
    "        left_on=\"id\",\n",
    "        right_on=\"parent\",\n",
    "    )\n",
    "    children_description.dropna(axis=0, subset=[\"children_description\"], inplace=True)\n",
    "    children_description = children_description.groupby([\"id\"])[\n",
    "        \"children_description\"\n",
    "    ].agg(list)\n",
    "    children_description = children_description.apply(lambda x: \" \".join(x))\n",
    "\n",
    "    topics = topics.merge(\n",
    "        children_description,\n",
    "        how=\"left\",\n",
    "        left_on=\"id\",\n",
    "        right_index=True,\n",
    "    )\n",
    "\n",
    "    topics_df[\"title\"].fillna(\"\", inplace=True)\n",
    "    topics[\"breadcrumbs\"] = topics[\"id\"].apply(lambda x: Topic(x).get_breadcrumbs())\n",
    "\n",
    "    topics[\"feature\"] = topics[\n",
    "        [\n",
    "            \"title\",\n",
    "            \"description\",\n",
    "            \"breadcrumbs\",\n",
    "            \"children_description\",\n",
    "            \"parent_description\",\n",
    "        ]\n",
    "    ].apply(lambda x: \" \".join(x.dropna().tolist()), axis=1)\n",
    "    topics = topics[[\"id\", \"language\", \"feature\"]]\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "def truncate(row, truncate_columns=[(\"description\", 100), (\"text\", 60)]):\n",
    "    if row[\"language\"] not in [\"en\", \"es\"]:\n",
    "        return row\n",
    "\n",
    "    for truncate_column in truncate_columns:\n",
    "        column_name = truncate_column[0]\n",
    "        max_length = truncate_column[1]\n",
    "\n",
    "        text = row[column_name]\n",
    "\n",
    "        if pd.isna(text):\n",
    "            continue\n",
    "\n",
    "        text = text.split()\n",
    "        if len(text) > max_length:\n",
    "            text = text[: max_length // 2] + text[-(max_length // 2) :]\n",
    "            text = \" \".join(text)\n",
    "\n",
    "            row[column_name] = text\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def create_feature_for_content(content):\n",
    "    content = content.apply(truncate, axis=1)\n",
    "    content[\"feature\"] = content[\n",
    "        [\n",
    "            \"title\",\n",
    "            \"description\",\n",
    "            \"text\",\n",
    "        ]\n",
    "    ].apply(lambda x: \" \".join(x.dropna().tolist()), axis=1)\n",
    "\n",
    "    content = content[[\"id\", \"language\", \"feature\"]]\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:47:10.478179Z",
     "iopub.status.busy": "2023-03-12T02:47:10.477374Z",
     "iopub.status.idle": "2023-03-12T02:47:10.517167Z",
     "shell.execute_reply": "2023-03-12T02:47:10.516274Z",
     "shell.execute_reply.started": "2023-03-12T02:47:10.478152Z"
    }
   },
   "outputs": [],
   "source": [
    "# submit のため前処理\n",
    "sample_submission.drop(columns=\"content_ids\", inplace=True)\n",
    "sample_submission = sample_submission.merge(\n",
    "    topics,\n",
    "    how=\"left\",\n",
    "    left_on=\"topic_id\",\n",
    "    right_on=\"id\",\n",
    ")\n",
    "sample_submission.drop(columns=\"topic_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:47:10.520108Z",
     "iopub.status.busy": "2023-03-12T02:47:10.519395Z",
     "iopub.status.idle": "2023-03-12T02:49:31.913025Z",
     "shell.execute_reply": "2023-03-12T02:49:31.911945Z",
     "shell.execute_reply.started": "2023-03-12T02:47:10.520071Z"
    }
   },
   "outputs": [],
   "source": [
    "# retriver 用にデータを作る\n",
    "topics = create_feature_for_topics(topics)\n",
    "sample_submission = create_feature_for_topics(sample_submission)\n",
    "\n",
    "content = create_feature_for_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:49:32.218493Z",
     "iopub.status.busy": "2023-03-12T02:49:32.217816Z",
     "iopub.status.idle": "2023-03-12T02:49:32.231887Z",
     "shell.execute_reply": "2023-03-12T02:49:32.230875Z",
     "shell.execute_reply.started": "2023-03-12T02:49:32.218454Z"
    }
   },
   "outputs": [],
   "source": [
    "# retriver の実装。topics は has_content == True のみの想定\n",
    "def retrieve(topics, content, path_to_weights):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = SentenceTransformer(path_to_weights, device=device)\n",
    "\n",
    "    correlations_preds = []\n",
    "    for language, topics_language in topics.groupby([\"language\"]):\n",
    "        content_language = content[content[\"language\"] == language]\n",
    "        content_language.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        topics_language_embeddings = model.encode(\n",
    "            topics_language[\"feature\"].tolist(), device=device\n",
    "        )\n",
    "        content_language_embeddings = model.encode(\n",
    "            content_language[\"feature\"].tolist(), device=device\n",
    "        )\n",
    "\n",
    "        neigh = NearestNeighbors(n_neighbors=100, metric=\"cosine\")\n",
    "        neigh.fit(content_language_embeddings)\n",
    "\n",
    "        distances, indices = neigh.kneighbors(topics_language_embeddings)\n",
    "\n",
    "        distances = distances.flatten()\n",
    "        indices = indices.flatten()\n",
    "\n",
    "        distances = distances.reshape((-1, 1))\n",
    "        indices = indices.reshape((-1, 1))\n",
    "\n",
    "        topics_language_id = topics_language[\"id\"].to_numpy()\n",
    "        topics_language_id = np.repeat(topics_language_id, 100)\n",
    "        topics_language_id = topics_language_id.reshape((-1, 1))\n",
    "\n",
    "        correlations_pred = np.hstack((topics_language_id, indices, distances))\n",
    "        correlations_pred = pd.DataFrame(\n",
    "            correlations_pred, columns=[\"topic_id\", \"content_index\", \"distance\"]\n",
    "        )\n",
    "\n",
    "        correlations_pred = correlations_pred.merge(\n",
    "            content_language[\"id\"],\n",
    "            how=\"left\",\n",
    "            left_on=\"content_index\",\n",
    "            right_index=True,\n",
    "        )\n",
    "        correlations_pred.rename(columns={\"id\": \"content_id\"}, inplace=True)\n",
    "        correlations_pred = correlations_pred[[\"topic_id\", \"content_id\", \"distance\"]]\n",
    "\n",
    "        correlations_preds.append(correlations_pred)\n",
    "\n",
    "    correlations_preds = pd.concat(correlations_preds)\n",
    "\n",
    "    return correlations_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:49:32.233858Z",
     "iopub.status.busy": "2023-03-12T02:49:32.233470Z",
     "iopub.status.idle": "2023-03-12T02:49:32.245999Z",
     "shell.execute_reply": "2023-03-12T02:49:32.245053Z",
     "shell.execute_reply.started": "2023-03-12T02:49:32.233818Z"
    }
   },
   "outputs": [],
   "source": [
    "# recall 計算用の関数\n",
    "def calculate_recall(corr_true, corr_pred, print_recall):\n",
    "    corr_true_copy = corr_true.copy()\n",
    "    corr_pred_copy = corr_pred.copy()\n",
    "\n",
    "    corr_true_copy = corr_true_copy[\n",
    "        corr_true_copy[\"topic_id\"].isin(corr_pred_copy[\"topic_id\"])\n",
    "    ]\n",
    "\n",
    "    corr_true_copy[\"content_ids\"] = corr_true_copy[\"content_ids\"].str.split()\n",
    "    corr_true_copy = corr_true_copy.explode(\"content_ids\")\n",
    "\n",
    "    corr_true_copy = corr_true_copy.merge(\n",
    "        corr_pred_copy,\n",
    "        how=\"left\",\n",
    "        left_on=[\"topic_id\", \"content_ids\"],\n",
    "        right_on=[\"topic_id\", \"content_id\"],\n",
    "    )\n",
    "\n",
    "    rowwise_len = len(corr_true_copy)\n",
    "    rowwise_recall = corr_true_copy[\"content_id\"].notnull().sum() / rowwise_len\n",
    "\n",
    "    if not print_recall:\n",
    "        return rowwise_recall\n",
    "\n",
    "    print(f\"（行単位で見たときの）再現率: {rowwise_recall:.2%}\")\n",
    "\n",
    "    corr_true_copy = corr_true_copy.groupby([\"topic_id\"]).agg(\n",
    "        {\"content_ids\": \"count\", \"content_id\": \"count\"}\n",
    "    )\n",
    "    corr_true_copy[\"recall\"] = (\n",
    "        corr_true_copy[\"content_id\"] / corr_true_copy[\"content_ids\"]\n",
    "    )\n",
    "\n",
    "    topicwise_len = len(corr_true_copy)\n",
    "\n",
    "    print(f\"すべての正解データを含む: {(corr_true_copy['recall'] == 1).sum() / topicwise_len:.2%}\")\n",
    "    print(\n",
    "        f\"正解データを 80% 以上含む: {(corr_true_copy['recall'] >= 0.8).sum() / topicwise_len:.2%}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"正解データを 50% 以上含む: {(corr_true_copy['recall'] >= 0.5).sum() / topicwise_len:.2%}\"\n",
    "    )\n",
    "    print(f\"正解データを１つも含まない: {(corr_true_copy['recall'] == 0).sum() / topicwise_len:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:49:32.248394Z",
     "iopub.status.busy": "2023-03-12T02:49:32.248038Z",
     "iopub.status.idle": "2023-03-12T02:49:32.266243Z",
     "shell.execute_reply": "2023-03-12T02:49:32.265234Z",
     "shell.execute_reply.started": "2023-03-12T02:49:32.248354Z"
    }
   },
   "outputs": [],
   "source": [
    "# retriver 学習用の関数\n",
    "def train_retriver(topics_train, topics_val, content, correlations):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "    correlations_copy = correlations.copy()\n",
    "    correlations_copy[\"content_ids\"] = correlations_copy[\"content_ids\"].str.split()\n",
    "    correlations_copy = correlations_copy.explode(\"content_ids\")\n",
    "    correlations_copy.rename(columns={\"content_ids\": \"content_id\"}, inplace=True)\n",
    "\n",
    "    topics_train = topics_train.merge(\n",
    "        correlations_copy,\n",
    "        how=\"left\",\n",
    "        left_on=\"id\",\n",
    "        right_on=\"topic_id\",\n",
    "    )\n",
    "    topics_train.rename(columns={\"feature\": \"topic_feature\"}, inplace=True)\n",
    "    topics_train.drop(columns=\"topic_id\", inplace=True)\n",
    "\n",
    "    topics_train = topics_train.merge(\n",
    "        content[[\"id\", \"feature\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"content_id\",\n",
    "        right_on=\"id\",\n",
    "        suffixes=(\"\", \"_\"),\n",
    "    )\n",
    "    topics_train.rename(columns={\"feature\": \"content_feature\"}, inplace=True)\n",
    "    topics_train.drop(columns=\"id_\", inplace=True)\n",
    "\n",
    "    train_examples = []\n",
    "    for row in topics_train.itertuples():\n",
    "        input_example = InputExample(texts=[row.topic_feature, row.content_feature])\n",
    "\n",
    "        train_examples.append(input_example)\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=256)\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "    correlations_preds = retrieve(topics_val, content, path_to_weights)\n",
    "\n",
    "    correlations_copy[\"is_match\"] = 1\n",
    "    correlations_preds = correlations_preds.merge(\n",
    "        correlations_copy,\n",
    "        how=\"left\",\n",
    "        on=[\"topic_id\", \"content_id\"],\n",
    "    )\n",
    "    correlations_preds[\"is_match\"].fillna(0, inplace=True)\n",
    "\n",
    "    correlations_preds = correlations_preds.merge(\n",
    "        topics_val[[\"id\", \"feature\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"topic_id\",\n",
    "        right_on=\"id\",\n",
    "    )\n",
    "    correlations_preds.rename(columns={\"feature\": \"topic_feature\"}, inplace=True)\n",
    "    correlations_preds.drop(columns=\"id\", inplace=True)\n",
    "\n",
    "    correlations_preds = correlations_preds.merge(\n",
    "        content[[\"id\", \"feature\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"content_id\",\n",
    "        right_on=\"id\",\n",
    "    )\n",
    "    correlations_preds.rename(columns={\"feature\": \"content_feature\"}, inplace=True)\n",
    "    correlations_preds.drop(columns=\"id\", inplace=True)\n",
    "\n",
    "    print(len(correlations_preds[correlations_preds[\"is_match\"] == 0]))\n",
    "    print(len(correlations_preds[correlations_preds[\"is_match\"] == 1]))\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    correlations_preds, _ = rus.fit_resample(\n",
    "        correlations_preds, correlations_preds[\"is_match\"]\n",
    "    )\n",
    "\n",
    "    print(len(correlations_preds[correlations_preds[\"is_match\"] == 0]))\n",
    "    print(len(correlations_preds[correlations_preds[\"is_match\"] == 1]))\n",
    "\n",
    "    topics = correlations_preds[\"topic_feature\"].tolist()\n",
    "    content = correlations_preds[\"content_feature\"].tolist()\n",
    "    labels = correlations_preds[\"is_match\"].tolist()\n",
    "\n",
    "    evaluator = evaluation.BinaryClassificationEvaluator(topics, content, labels)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=5,\n",
    "        evaluation_steps=10_000,\n",
    "        output_path=\"重みファイルの保存先を入力\",\n",
    "        use_amp=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:49:32.271498Z",
     "iopub.status.busy": "2023-03-12T02:49:32.271031Z",
     "iopub.status.idle": "2023-03-12T02:49:32.279419Z",
     "shell.execute_reply": "2023-03-12T02:49:32.278535Z",
     "shell.execute_reply.started": "2023-03-12T02:49:32.271471Z"
    }
   },
   "outputs": [],
   "source": [
    "# # retriver の学習を実行\n",
    "# train_retriver(\n",
    "#     topics_pipeline_train, topics_pipeline_val, content, correlations\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:49:32.281631Z",
     "iopub.status.busy": "2023-03-12T02:49:32.281025Z",
     "iopub.status.idle": "2023-03-12T02:51:41.284997Z",
     "shell.execute_reply": "2023-03-12T02:51:41.283753Z",
     "shell.execute_reply.started": "2023-03-12T02:49:32.281595Z"
    }
   },
   "outputs": [],
   "source": [
    "# retriver の実行と精度の確認\n",
    "correlations_preds = retrieve(sample_submission, content, PATH_TO_RETRIEVER_WEIGHTS)\n",
    "calculate_recall(correlations, correlations_preds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:41.994917Z",
     "iopub.status.busy": "2023-03-12T02:51:41.994554Z",
     "iopub.status.idle": "2023-03-12T02:51:42.006262Z",
     "shell.execute_reply": "2023-03-12T02:51:42.004973Z",
     "shell.execute_reply.started": "2023-03-12T02:51:41.994879Z"
    }
   },
   "outputs": [],
   "source": [
    "# F2 score の計算と閾値の最適化を行う関数\n",
    "def _f2_score(row):\n",
    "    true_content_ids = row[\"content_ids\"].split()\n",
    "    pred_content_ids = row[\"content_id\"]\n",
    "\n",
    "    true_content_ids = set(true_content_ids)\n",
    "    pred_content_ids = set(pred_content_ids)\n",
    "\n",
    "    tp = len(true_content_ids.intersection(pred_content_ids))\n",
    "    fp = len(pred_content_ids - true_content_ids)\n",
    "    fn = len(true_content_ids - pred_content_ids)\n",
    "\n",
    "    f2 = 0\n",
    "\n",
    "    if tp != 0:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f2 = (5 * precision * recall) / ((4 * precision) + recall)\n",
    "\n",
    "    return f2\n",
    "\n",
    "\n",
    "def f2_score(x, corr_true, corr_pred):\n",
    "    corr_true_copy = corr_true.copy()\n",
    "    corr_pred_copy = corr_pred.copy()\n",
    "\n",
    "    if x:\n",
    "        corr_pred_copy = corr_pred_copy[corr_pred_copy[\"probability\"] > x[0]]\n",
    "\n",
    "    corr_pred_copy = corr_pred_copy[[\"topic_id\", \"content_id\"]]\n",
    "    corr_pred_copy = corr_pred_copy.groupby(\"topic_id\").agg(list).reset_index()\n",
    "\n",
    "    corr_pred_copy = corr_pred_copy.merge(\n",
    "        corr_true_copy,\n",
    "        how=\"left\",\n",
    "        on=\"topic_id\",\n",
    "    )\n",
    "\n",
    "    corr_pred_copy[\"f2_score\"] = corr_pred_copy.apply(lambda x: _f2_score(x), axis=1)\n",
    "    f2_score = corr_pred_copy[\"f2_score\"].mean()\n",
    "\n",
    "    return -f2_score\n",
    "\n",
    "\n",
    "def optimize_threshold(corr_true, corr_pred, x0=[0.5]):\n",
    "    corr_true_copy = corr_true.copy()\n",
    "    corr_pred_copy = corr_pred.copy()\n",
    "\n",
    "    res = minimize(\n",
    "        f2_score, x0, args=(corr_true_copy, corr_pred_copy), method=\"Nelder-Mead\"\n",
    "    )\n",
    "\n",
    "    corr_pred_copy = corr_pred_copy[corr_pred_copy[\"probability\"] > res.x[0]]\n",
    "    f2_score_ = -f2_score(None, corr_true_copy, corr_pred_copy)\n",
    "\n",
    "    return f2_score_, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:42.008612Z",
     "iopub.status.busy": "2023-03-12T02:51:42.007874Z",
     "iopub.status.idle": "2023-03-12T02:51:42.094118Z",
     "shell.execute_reply": "2023-03-12T02:51:42.093076Z",
     "shell.execute_reply.started": "2023-03-12T02:51:42.008573Z"
    }
   },
   "outputs": [],
   "source": [
    "# reranker 用にデータを作る\n",
    "correlations_preds = correlations_preds.merge(\n",
    "    topics[[\"id\", \"feature\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"topic_id\",\n",
    "    right_on=\"id\",\n",
    ")\n",
    "correlations_preds.rename(columns={\"feature\": \"topic_feature\"}, inplace=True)\n",
    "correlations_preds.drop(columns=\"id\", inplace=True)\n",
    "\n",
    "correlations_preds = correlations_preds.merge(\n",
    "    content[[\"id\", \"feature\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"content_id\",\n",
    "    right_on=\"id\",\n",
    ")\n",
    "correlations_preds.rename(columns={\"feature\": \"content_feature\"}, inplace=True)\n",
    "correlations_preds.drop(columns=\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:42.096346Z",
     "iopub.status.busy": "2023-03-12T02:51:42.095694Z",
     "iopub.status.idle": "2023-03-12T02:51:42.438285Z",
     "shell.execute_reply": "2023-03-12T02:51:42.437358Z",
     "shell.execute_reply.started": "2023-03-12T02:51:42.096303Z"
    }
   },
   "outputs": [],
   "source": [
    "del topics, topics_df, topics_pipeline_train, topics_pipeline_val, content, correlations\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:42.440665Z",
     "iopub.status.busy": "2023-03-12T02:51:42.440230Z",
     "iopub.status.idle": "2023-03-12T02:51:42.454262Z",
     "shell.execute_reply": "2023-03-12T02:51:42.452997Z",
     "shell.execute_reply.started": "2023-03-12T02:51:42.440627Z"
    }
   },
   "outputs": [],
   "source": [
    "# reranker 学習用の関数\n",
    "def train_reranker(correlations_train, correlations_val, correlations_true):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = CrossEncoder(\"cross-encoder/stsb-roberta-base\", device=device)\n",
    "\n",
    "    correlations_train_copy = correlations_train.copy()\n",
    "    correlations_val_copy = correlations_val.copy()\n",
    "    correlations_true_copy = correlations_true.copy()\n",
    "\n",
    "    correlations_true_copy[\"content_ids\"] = correlations_true_copy[\n",
    "        \"content_ids\"\n",
    "    ].str.split()\n",
    "    correlations_true_copy = correlations_true_copy.explode(\"content_ids\")\n",
    "    correlations_true_copy.rename(columns={\"content_ids\": \"content_id\"}, inplace=True)\n",
    "\n",
    "    correlations_true_copy[\"is_match\"] = 1\n",
    "    correlations_train_copy = correlations_train_copy.merge(\n",
    "        correlations_true_copy,\n",
    "        how=\"left\",\n",
    "        on=[\"topic_id\", \"content_id\"],\n",
    "    )\n",
    "    correlations_train_copy[\"is_match\"].fillna(0, inplace=True)\n",
    "\n",
    "    train_examples = []\n",
    "    for row in correlations_train_copy.itertuples():\n",
    "        input_example = InputExample(\n",
    "            texts=[row.topic_feature, row.content_feature], label=row.is_match\n",
    "        )\n",
    "\n",
    "        train_examples.append(input_example)\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)\n",
    "\n",
    "    correlations_val_copy = correlations_val_copy.merge(\n",
    "        correlations_true_copy,\n",
    "        how=\"left\",\n",
    "        on=[\"topic_id\", \"content_id\"],\n",
    "    )\n",
    "    correlations_val_copy[\"is_match\"].fillna(0, inplace=True)\n",
    "\n",
    "    print(len(correlations_val_copy[correlations_val_copy[\"is_match\"] == 0]))\n",
    "    print(len(correlations_val_copy[correlations_val_copy[\"is_match\"] == 1]))\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    correlations_val_copy, _ = rus.fit_resample(\n",
    "        correlations_val_copy, correlations_val_copy[\"is_match\"]\n",
    "    )\n",
    "\n",
    "    print(len(correlations_val_copy[correlations_val_copy[\"is_match\"] == 0]))\n",
    "    print(len(correlations_val_copy[correlations_val_copy[\"is_match\"] == 1]))\n",
    "\n",
    "    topics = correlations_val_copy[\"topic_feature\"].to_numpy()\n",
    "    content = correlations_val_copy[\"content_feature\"].to_numpy()\n",
    "\n",
    "    topics = topics.reshape((-1, 1))\n",
    "    content = content.reshape((-1, 1))\n",
    "\n",
    "    topics_content = np.hstack((topics, content))\n",
    "    topics_content = topics_content.tolist()\n",
    "\n",
    "    labels = correlations_val_copy[\"is_match\"].tolist()\n",
    "\n",
    "    evaluator = CEBinaryClassificationEvaluator(topics_content, labels)\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        evaluator=evaluator,\n",
    "        epochs=1,\n",
    "        evaluation_steps=10_000,\n",
    "        output_path=\"重みファイルの保存先を入力\",\n",
    "        use_amp=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:42.456972Z",
     "iopub.status.busy": "2023-03-12T02:51:42.456179Z",
     "iopub.status.idle": "2023-03-12T02:51:42.468757Z",
     "shell.execute_reply": "2023-03-12T02:51:42.467687Z",
     "shell.execute_reply.started": "2023-03-12T02:51:42.456931Z"
    }
   },
   "outputs": [],
   "source": [
    "# # reranker の学習を実行\n",
    "# train_reranker(correlations_preds_train, correlations_preds_val, correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:42.470638Z",
     "iopub.status.busy": "2023-03-12T02:51:42.470129Z",
     "iopub.status.idle": "2023-03-12T02:51:42.479128Z",
     "shell.execute_reply": "2023-03-12T02:51:42.478065Z",
     "shell.execute_reply.started": "2023-03-12T02:51:42.470602Z"
    }
   },
   "outputs": [],
   "source": [
    "# reranker の実装\n",
    "def rerank(correlations_preds, path_to_weights):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = CrossEncoder(path_to_weights, device=device)\n",
    "    \n",
    "    topics = correlations_preds[\"topic_feature\"].to_numpy()\n",
    "    content = correlations_preds[\"content_feature\"].to_numpy()\n",
    "    \n",
    "    topics = topics.reshape((-1, 1))\n",
    "    content = content.reshape((-1, 1))\n",
    "\n",
    "    topics_content = np.hstack((topics, content))\n",
    "    topics_content = topics_content.tolist()\n",
    "    \n",
    "    correlations_preds[\"reranker_probability\"] = model.predict(topics_content)\n",
    "    \n",
    "    return correlations_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T02:51:42.480933Z",
     "iopub.status.busy": "2023-03-12T02:51:42.480565Z",
     "iopub.status.idle": "2023-03-12T02:51:50.259840Z",
     "shell.execute_reply": "2023-03-12T02:51:50.258874Z",
     "shell.execute_reply.started": "2023-03-12T02:51:42.480896Z"
    }
   },
   "outputs": [],
   "source": [
    "correlations_preds = rerank(correlations_preds, PATH_TO_RERANKER_WEIGHTS)\n",
    "\n",
    "correlations_preds.loc[correlations_preds[\"reranker_probability\"] <= 0.28320312, \"content_id\"] = np.nan\n",
    "\n",
    "correlations_preds = correlations_preds[[\"topic_id\", \"content_id\"]]\n",
    "correlations_preds = correlations_preds.groupby([\"topic_id\"]).agg(list).reset_index()\n",
    "correlations_preds[\"content_id\"] = correlations_preds[\"content_id\"].apply(lambda x: [x_ for x_ in x if pd.notna(x_)])\n",
    "correlations_preds[\"content_id\"] = correlations_preds[\"content_id\"].str.join(\" \")\n",
    "\n",
    "correlations_preds.rename(columns={\"content_id\": \"content_ids\"}, inplace=True)\n",
    "\n",
    "correlations_preds.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
